{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXaL7nU6TEsV"
      },
      "source": [
        "# MakeItTalk Quick Demo (natural human face animation)\n",
        "\n",
        "- included project setup + pretrained model download\n",
        "- provides step-by-step details\n",
        "- todo: tdlr version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owgbZ22TQmz"
      },
      "source": [
        "## Preparations\n",
        "- Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 20271,
          "status": "ok",
          "timestamp": 1679425942496,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "fvZT10MpjPMf",
        "outputId": "65999542-2cf5-4aee-8791-23e97cdbdbf0"
      },
      "outputs": [],
      "source": [
        "!pip uninstall librosa\n",
        "!pip install librosa==0.8.1\n",
        "\n",
        "import librosa\n",
        "\n",
        "print(librosa.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1234,
          "status": "ok",
          "timestamp": 1679425955001,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "yB-ixde4R3nO",
        "outputId": "950f3951-e35a-40e0-b5e2-3d565f3f5928"
      },
      "outputs": [],
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getoutput('nvidia-smi'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o31a6SpeTXDM"
      },
      "source": [
        "- Check ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2038,
          "status": "ok",
          "timestamp": 1679425961051,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "u4EcdzstSB71",
        "outputId": "dc6bb595-ec42-44d7-84f2-f89718798c22"
      },
      "outputs": [],
      "source": [
        "print(subprocess.getoutput('ffmpeg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taPSDYiSTcM_"
      },
      "source": [
        "- Install Github https://github.com/yzhou359/MakeItTalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2174,
          "status": "ok",
          "timestamp": 1679425970003,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "4G0XLqo4SofV",
        "outputId": "9fd0b464-d53a-4c3f-c6e6-d698b37dfad1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/yzhou359/MakeItTalk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xe5u4Ede-G5"
      },
      "source": [
        "- Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 114532,
          "status": "ok",
          "timestamp": 1679426088809,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "sR4ExzplfBHk",
        "outputId": "de5e3eaa-fd38-431e-cf40-55ffcd0de05e"
      },
      "outputs": [],
      "source": [
        "%cd MakeItTalk/\n",
        "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
        "!pip install -r requirements.txt\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sed -i 's/mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T/mel(sr=16000, n_fft=1024, fmin=90, fmax=7600, n_mels=80).T/' /content/MakeItTalk/src/autovc/retrain_version/vocoder_spec/extract_f0_func.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "executionInfo": {
          "elapsed": 23317,
          "status": "error",
          "timestamp": 1679133212044,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "hSjFGF8qqYrW",
        "outputId": "3bb9fb4d-84bd-4600-cb53-9ae7765b15a9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AByGGO5fd14P"
      },
      "source": [
        "- Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 921,
          "status": "ok",
          "timestamp": 1679426167084,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "NYRS53wbpqlH",
        "outputId": "99b6989a-b12f-4001-f644-3326da536cb4"
      },
      "outputs": [],
      "source": [
        "# !gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x \n",
        "!cp /content/drive/MyDrive/AI/ckpt_autovc.pth examples/ckpt/ckpt_autovc.pth\n",
        "!cp /content/drive/MyDrive/AI/ckpt_116_i2i_comb.pth examples/ckpt/ckpt_116_i2i_comb.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 52515,
          "status": "ok",
          "timestamp": 1679426223646,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "SU4abC3iTmXA",
        "outputId": "963649e1-68a0-4f78-8ec5-980e358d84ba"
      },
      "outputs": [],
      "source": [
        "!mkdir examples/dump\n",
        "!mkdir examples/ckpt\n",
        "!pip install gdown\n",
        "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
        "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
        "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
        "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
        "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn7YeVqxppT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37JeD3ZZdI-a"
      },
      "source": [
        "- prepare your images/audios (or you can use the existing ones)\n",
        "  - An image to animate: upload to `MakeItTalk/examples` folder, image size should be 256x256\n",
        "  - An audio (hopefully no noise) to talk: upload to `MakeItTalk/examples` folder as well\n",
        "\n",
        "## Step 0: import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8532,
          "status": "ok",
          "timestamp": 1679426245508,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "olj6VcfiTrd_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from src.approaches.train_image_translation import Image_translation_block\n",
        "import torch\n",
        "import pickle\n",
        "import face_alignment\n",
        "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
        "import shutil\n",
        "import time\n",
        "import util.utils as util\n",
        "from scipy.signal import savgol_filter\n",
        "from src.approaches.train_audio2landmark import Audio2landmark_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8aaCE6vgmXy"
      },
      "source": [
        "## Step 1: Basic setup for the animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1679430913774,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "58s-c9H8dWPW"
      },
      "outputs": [],
      "source": [
        "default_head_name = 'girl_detail'           # the image name (with no .jpg) to animate\n",
        "ADD_NAIVE_EYE = True                 # whether add naive eye blink\n",
        "CLOSE_INPUT_FACE_MOUTH = False       # if your image has an opened mouth, put this as True, else False\n",
        "AMP_LIP_SHAPE_X = 2.                 # amplify the lip motion in horizontal direction\n",
        "AMP_LIP_SHAPE_Y = 2.                 # amplify the lip motion in vertical direction\n",
        "AMP_HEAD_POSE_MOTION = 0.7           # amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 26059,
          "status": "ok",
          "timestamp": 1679426281176,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "ycHUvvJQ6fbt",
        "outputId": "0955b2f5-eb97-4df4-bc29-8bd59ae7f516"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 924,
          "status": "ok",
          "timestamp": 1679431001261,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "RvB9eeUc74WD"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# 打开原始图片\n",
        "image = Image.open(\"/content/MakeItTalk/examples/girl.jpg\")\n",
        "\n",
        "# 设置缩放比例\n",
        "scale = 0.7  # 50%缩放\n",
        "\n",
        "# 计算新的大小\n",
        "new_width = int(image.width * scale)\n",
        "new_height = int(image.height * scale)\n",
        "\n",
        "# 缩放图片\n",
        "image.thumbnail((new_width, new_height))\n",
        "\n",
        "# 保存缩放后的图片\n",
        "image.save(\"/content/MakeItTalk/examples/girl_0.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5413,
          "status": "ok",
          "timestamp": 1679426372097,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "vylRT3d1Qnm8",
        "outputId": "5a455d37-bc4f-49cc-99cc-27595d394691"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 652,
          "status": "ok",
          "timestamp": 1679431009023,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "GozQDKJD6WlJ",
        "outputId": "102b3191-4dfb-43cd-9c37-b1c8aff5fdd7"
      },
      "outputs": [],
      "source": [
        "import face_recognition\n",
        "from PIL import Image\n",
        "\n",
        "# 加载图片\n",
        "image = face_recognition.load_image_file(\"/content/MakeItTalk/examples/girl_0.jpg\")\n",
        "\n",
        "# 定位脸部位置\n",
        "face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "left=face_locations[0][3]-100\n",
        "top=face_locations[0][0]-50\n",
        "right=face_locations[0][3]+156\n",
        "bottom=face_locations[0][0]+206\n",
        "\n",
        "# 裁剪出脸部图片\n",
        "face_image = Image.fromarray(image).crop((left, top, right, bottom))\n",
        "print(face_locations[0][3], face_locations[0][0], face_locations[0][1], face_locations[0][2])\n",
        "print(right, top, left, bottom)\n",
        "print(right-left, bottom-top)\n",
        "# !identify /content/MakeItTalk/examples/girl_0.jpg\n",
        "# 保存脸部图像\n",
        "face_image.save(\"/content/MakeItTalk/examples/girl_detail.jpg\")\n",
        "\n",
        "# 将脸部图片粘贴到原始图片上\n",
        "image_pil = Image.fromarray(image)\n",
        "image_pil.paste(face_image, (face_locations[0][3], face_locations[0][0]))\n",
        "\n",
        "# 保存新的图片\n",
        "image_pil.save(\"/content/MakeItTalk/examples/girl_1.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1883,
          "status": "ok",
          "timestamp": 1677918240201,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "ifxd50tkuFBt",
        "outputId": "163d78c4-471f-4849-db83-baf4f3ce7fab"
      },
      "outputs": [],
      "source": [
        "!git reset --hard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1319,
          "status": "ok",
          "timestamp": 1679426386538,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "OTs4h1IO8zTV",
        "outputId": "2bb7f977-eb76-4f00-9e14-7381682831a0"
      },
      "outputs": [],
      "source": [
        "!identify /content/MakeItTalk/examples/girl_0.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 0,
          "status": "ok",
          "timestamp": 1679426410834,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "x049MFYI3YRh",
        "outputId": "ad0792a7-39e9-455c-b1ae-dd2711b61a1c"
      },
      "outputs": [],
      "source": [
        "!apt-get install imagemagick\n",
        "!identify /content/MakeItTalk/examples/girl.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRFBOqXMguSH"
      },
      "source": [
        "Default hyper-parameters for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 564,
          "status": "ok",
          "timestamp": 1679431144220,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "ZkZRYLSCf8TK"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--jpg', type=str, default='{}.jpg'.format(default_head_name))\n",
        "parser.add_argument('--close_input_face_mouth', default=CLOSE_INPUT_FACE_MOUTH, action='store_true')\n",
        "\n",
        "parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
        "parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth')\n",
        "parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
        "parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
        "\n",
        "parser.add_argument('--amp_lip_x', type=float, default=AMP_LIP_SHAPE_X)\n",
        "parser.add_argument('--amp_lip_y', type=float, default=AMP_LIP_SHAPE_Y)\n",
        "parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
        "parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
        "parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
        "parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
        "parser.add_argument('--output_folder', type=str, default='examples')\n",
        "\n",
        "parser.add_argument('--test_end2end', default=True, action='store_true')\n",
        "parser.add_argument('--dump_dir', type=str, default='', help='')\n",
        "parser.add_argument('--pos_dim', default=7, type=int)\n",
        "parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
        "parser.add_argument('--transformer_d_model', default=32, type=int)\n",
        "parser.add_argument('--transformer_N', default=2, type=int)\n",
        "parser.add_argument('--transformer_heads', default=2, type=int)\n",
        "parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
        "parser.add_argument('--init_content_encoder', type=str, default='')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
        "parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
        "parser.add_argument('--write', default=False, action='store_true')\n",
        "parser.add_argument('--segment_batch_size', type=int, default=1, help='batch size')\n",
        "parser.add_argument('--emb_coef', default=3.0, type=float)\n",
        "parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
        "parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "opt_parser = parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 902,
          "status": "ok",
          "timestamp": 1679426421750,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "KtuWUmJa13B8",
        "outputId": "fe8698ca-e189-4141-dec1-3a0bdd64d91e"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i /content/MakeItTalk/examples/girl.png /content/MakeItTalk/examples/girl.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1060,
          "status": "ok",
          "timestamp": 1679426441343,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "PzqTQNwC4o-G",
        "outputId": "7683cc34-8f11-45ed-88a2-e466c063f8b5"
      },
      "outputs": [],
      "source": [
        "!identify /content/MakeItTalk/examples/hermione.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchIUwTTg3AB"
      },
      "source": [
        "## Step 2: load the image and detect its landmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 获取 LandmarksType 枚举的所有成员\n",
        "landmarks_type_enum = face_alignment.LandmarksType\n",
        "members = [member.name for member in landmarks_type_enum]\n",
        "\n",
        "# 打印所有成员\n",
        "print(members)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 15723,
          "status": "ok",
          "timestamp": 1679431409690,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "SmYcSmrugxQK"
      },
      "outputs": [],
      "source": [
        "img =cv2.imread('examples/' + opt_parser.jpg)\n",
        "predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True)\n",
        "shapes = predictor.get_landmarks(img)\n",
        "if (not shapes or len(shapes) != 1):\n",
        "    print('Cannot detect face landmarks. Exit.')\n",
        "    exit(-1)\n",
        "shape_3d = shapes[0]\n",
        "\n",
        "if(opt_parser.close_input_face_mouth):\n",
        "    util.close_input_face_mouth(shape_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_9LmmACg9Mq"
      },
      "source": [
        "## (Optional) Simple manual adjustment to landmarks in case FAN is not accurate, e.g.\n",
        "- slimmer lips\n",
        "- wider eyes\n",
        "- wider mouth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "executionInfo": {
          "elapsed": 545,
          "status": "ok",
          "timestamp": 1679431412904,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "R2PLXNlhgztJ"
      },
      "outputs": [],
      "source": [
        "shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * 1.05 + np.mean(shape_3d[48:, 0]) # wider lips\n",
        "shape_3d[49:54, 1] += 0.           # thinner upper lip\n",
        "shape_3d[55:60, 1] -= 1.           # thinner lower lip\n",
        "shape_3d[[37,38,43,44], 1] -=2.    # larger eyes\n",
        "shape_3d[[40,41,46,47], 1] +=2.    # larger eyes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nlaLLoShR1k"
      },
      "source": [
        "Normalize face as input to audio branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1679431414873,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "W0GkD0fThN-2"
      },
      "outputs": [],
      "source": [
        "shape_3d, scale, shift = util.norm_input_face(shape_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAcGrT3PhY3T"
      },
      "source": [
        "## Step 3: Generate input data for inference based on uploaded audio `MakeItTalk/examples/*.wav`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 682,
          "status": "ok",
          "timestamp": 1679431417646,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "Mqh5A_7chQ8g",
        "outputId": "a291550e-449d-4207-9174-e96d830b38a6"
      },
      "outputs": [],
      "source": [
        "au_data = []\n",
        "au_emb = []\n",
        "ains = glob.glob1('examples', '*.wav')\n",
        "ains = [item for item in ains if item is not 'tmp.wav']\n",
        "ains.sort()\n",
        "for ain in ains:\n",
        "    os.system('ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
        "    shutil.copyfile('examples/tmp.wav', 'examples/{}'.format(ain))\n",
        "\n",
        "    # au embedding\n",
        "    from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
        "    me, ae = get_spk_emb('examples/{}'.format(ain))\n",
        "    au_emb.append(me.reshape(-1))\n",
        "\n",
        "    print('Processing audio file', ain)\n",
        "    c = AutoVC_mel_Convertor('examples')\n",
        "\n",
        "    au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', ain),\n",
        "           autovc_model_path=opt_parser.load_AUTOVC_name)\n",
        "    au_data += au_data_i\n",
        "if(os.path.isfile('examples/tmp.wav')):\n",
        "    os.remove('examples/tmp.wav')\n",
        "\n",
        "# landmark fake placeholder\n",
        "fl_data = []\n",
        "rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
        "for au, info in au_data:\n",
        "    au_length = au.shape[0]\n",
        "    fl = np.zeros(shape=(au_length, 68 * 3))\n",
        "    fl_data.append((fl, info))\n",
        "    rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
        "    rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
        "    anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
        "\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
        "if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
        "\n",
        "with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
        "    pickle.dump(fl_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
        "    pickle.dump(au_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
        "    gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
        "    pickle.dump(gaze, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "executionInfo": {
          "elapsed": 640,
          "status": "ok",
          "timestamp": 1679432065231,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "1nBW_VIYWWyf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " # Copyright 2020 Adobe\n",
        " # All Rights Reserved.\n",
        " \n",
        " # NOTICE: Adobe permits you to use, modify, and distribute this file in\n",
        " # accordance with the terms of the Adobe license agreement accompanying\n",
        " # it.\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "from src.models.model_image_translation import ResUnetGenerator, VGGLoss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os, glob\n",
        "from src.dataset.image_translation.image_translation_dataset import vis_landmark_on_img, vis_landmark_on_img98, vis_landmark_on_img74\n",
        "\n",
        "\n",
        "from thirdparty.AdaptiveWingLoss.core import models\n",
        "from thirdparty.AdaptiveWingLoss.utils.utils import get_preds_fromhm\n",
        "\n",
        "import face_alignment\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Image_translation_block1():\n",
        "\n",
        "    def __init__(self, opt_parser, single_test=False):\n",
        "        print('Run on device {}'.format(device))\n",
        "\n",
        "        # for key in vars(opt_parser).keys():\n",
        "        #     print(key, ':', vars(opt_parser)[key])\n",
        "        self.opt_parser = opt_parser\n",
        "\n",
        "        # model\n",
        "        if(opt_parser.add_audio_in):\n",
        "            self.G = ResUnetGenerator(input_nc=7, output_nc=3, num_downs=6, use_dropout=False)\n",
        "        else:\n",
        "            self.G = ResUnetGenerator(input_nc=6, output_nc=3, num_downs=6, use_dropout=False)\n",
        "\n",
        "        if (opt_parser.load_G_name != ''):\n",
        "            ckpt = torch.load(opt_parser.load_G_name)\n",
        "            try:\n",
        "                self.G.load_state_dict(ckpt['G'])\n",
        "            except:\n",
        "                tmp = nn.DataParallel(self.G)\n",
        "                tmp.load_state_dict(ckpt['G'])\n",
        "                self.G.load_state_dict(tmp.module.state_dict())\n",
        "                del tmp\n",
        "\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs in G mode!\")\n",
        "            self.G = nn.DataParallel(self.G)\n",
        "\n",
        "        self.G.to(device)\n",
        "\n",
        "        if(not single_test):\n",
        "            # dataset\n",
        "            if(opt_parser.use_vox_dataset == 'raw'):\n",
        "                if(opt_parser.comb_fan_awing):\n",
        "                    from src.dataset.image_translation.image_translation_dataset import \\\n",
        "                        image_translation_raw74_dataset as image_translation_dataset\n",
        "                elif(opt_parser.add_audio_in):\n",
        "                    from src.dataset.image_translation.image_translation_dataset import image_translation_raw98_with_audio_dataset as \\\n",
        "                        image_translation_dataset\n",
        "                else:\n",
        "                    from src.dataset.image_translation.image_translation_dataset import image_translation_raw98_dataset as \\\n",
        "                    image_translation_dataset\n",
        "            else:\n",
        "                from src.dataset.image_translation.image_translation_dataset import image_translation_preprocessed98_dataset as \\\n",
        "                    image_translation_dataset\n",
        "\n",
        "            self.dataset = image_translation_dataset(num_frames=opt_parser.num_frames)\n",
        "            self.dataloader = torch.utils.data.DataLoader(self.dataset,\n",
        "                                                          batch_size=opt_parser.batch_size,\n",
        "                                                          shuffle=True,\n",
        "                                                          num_workers=opt_parser.num_workers)\n",
        "\n",
        "            # criterion\n",
        "            self.criterionL1 = nn.L1Loss()\n",
        "            self.criterionVGG = VGGLoss()\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                print(\"Let's use\", torch.cuda.device_count(), \"GPUs in VGG model!\")\n",
        "                self.criterionVGG = nn.DataParallel(self.criterionVGG)\n",
        "            self.criterionVGG.to(device)\n",
        "\n",
        "            # optimizer\n",
        "            self.optimizer = torch.optim.Adam(self.G.parameters(), lr=opt_parser.lr, betas=(0.5, 0.999))\n",
        "\n",
        "            # writer\n",
        "            if(opt_parser.write):\n",
        "                self.writer = SummaryWriter(log_dir=os.path.join(opt_parser.log_dir, opt_parser.name))\n",
        "                self.count = 0\n",
        "\n",
        "            # ===========================================================\n",
        "            #       online landmark alignment : Awing\n",
        "            # ===========================================================\n",
        "            PRETRAINED_WEIGHTS = 'thirdparty/AdaptiveWingLoss/ckpt/WFLW_4HG.pth'\n",
        "            GRAY_SCALE = False\n",
        "            HG_BLOCKS = 4\n",
        "            END_RELU = False\n",
        "            NUM_LANDMARKS = 98\n",
        "\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model_ft = models.FAN(HG_BLOCKS, END_RELU, GRAY_SCALE, NUM_LANDMARKS)\n",
        "\n",
        "            checkpoint = torch.load(PRETRAINED_WEIGHTS)\n",
        "            if 'state_dict' not in checkpoint:\n",
        "                model_ft.load_state_dict(checkpoint)\n",
        "            else:\n",
        "                pretrained_weights = checkpoint['state_dict']\n",
        "                model_weights = model_ft.state_dict()\n",
        "                pretrained_weights = {k: v for k, v in pretrained_weights.items() \\\n",
        "                                      if k in model_weights}\n",
        "                model_weights.update(pretrained_weights)\n",
        "                model_ft.load_state_dict(model_weights)\n",
        "            print('Load AWing model sucessfully')\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                print(\"Let's use\", torch.cuda.device_count(), \"GPUs for AWing!\")\n",
        "                self.fa_model = nn.DataParallel(model_ft).to(self.device).eval()\n",
        "            else:\n",
        "                self.fa_model = model_ft.to(self.device).eval()\n",
        "\n",
        "            # ===========================================================\n",
        "            #       online landmark alignment : FAN\n",
        "            # ===========================================================\n",
        "            if(opt_parser.comb_fan_awing):\n",
        "                if(opt_parser.fan_2or3D == '2D'):\n",
        "                    self.predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D,\n",
        "                                                                  device='cuda' if torch.cuda.is_available() else \"cpu\",\n",
        "                                                                  flip_input=True)\n",
        "                else:\n",
        "                    self.predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D,\n",
        "                                                                  device='cuda' if torch.cuda.is_available() else \"cpu\",\n",
        "                                                                  flip_input=True)\n",
        "\n",
        "    def __train_pass__(self, epoch, is_training=True):\n",
        "        st_epoch = time.time()\n",
        "        if(is_training):\n",
        "            self.G.train()\n",
        "            status = 'TRAIN'\n",
        "        else:\n",
        "            self.G.eval()\n",
        "            status = 'EVAL'\n",
        "\n",
        "        g_time = 0.0\n",
        "        for i, batch in enumerate(self.dataloader):\n",
        "            if(i >= len(self.dataloader)-2):\n",
        "                break\n",
        "            st_batch = time.time()\n",
        "\n",
        "            if(self.opt_parser.comb_fan_awing):\n",
        "                image_in, image_out, fan_pred_landmarks = batch\n",
        "                fan_pred_landmarks = fan_pred_landmarks.reshape(-1, 68, 3).detach().cpu().numpy()\n",
        "            elif(self.opt_parser.add_audio_in):\n",
        "                image_in, image_out, audio_in = batch\n",
        "                audio_in = audio_in.reshape(-1, 1, 256, 256).to(device)\n",
        "            else:\n",
        "                image_in, image_out = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # # online landmark (AwingNet)\n",
        "                image_in, image_out = \\\n",
        "                    image_in.reshape(-1, 3, 256, 256).to(device), image_out.reshape(-1, 3, 256, 256).to(device)\n",
        "                inputs = image_out\n",
        "                outputs, boundary_channels = self.fa_model(inputs)\n",
        "                pred_heatmap = outputs[-1][:, :-1, :, :].detach().cpu()\n",
        "                pred_landmarks, _ = get_preds_fromhm(pred_heatmap)\n",
        "                pred_landmarks = pred_landmarks.numpy() * 4\n",
        "\n",
        "                # online landmark (FAN) -> replace jaw + eye brow in AwingNet\n",
        "                if(self.opt_parser.comb_fan_awing):\n",
        "                    fl_jaw_eyebrow = fan_pred_landmarks[:, 0:27, 0:2]\n",
        "                    fl_rest = pred_landmarks[:, 51:, :]\n",
        "                    pred_landmarks = np.concatenate([fl_jaw_eyebrow, fl_rest], axis=1).astype(np.int)\n",
        "\n",
        "            # draw landmark on while bg\n",
        "            img_fls = []\n",
        "            for pred_fl in pred_landmarks:\n",
        "                img_fl = np.ones(shape=(256, 256, 3)) * 255.0\n",
        "                if(self.opt_parser.comb_fan_awing):\n",
        "                    img_fl = vis_landmark_on_img74(img_fl, pred_fl)  # 74x2\n",
        "                else:\n",
        "                    img_fl = vis_landmark_on_img98(img_fl, pred_fl)  # 98x2\n",
        "                img_fls.append(img_fl.transpose((2, 0, 1)))\n",
        "            img_fls = np.stack(img_fls, axis=0).astype(np.float32) / 255.0\n",
        "            image_fls_in = torch.tensor(img_fls, requires_grad=False).to(device)\n",
        "\n",
        "            if(self.opt_parser.add_audio_in):\n",
        "                # print(image_fls_in.shape, image_in.shape, audio_in.shape)\n",
        "                image_in = torch.cat([image_fls_in, image_in, audio_in], dim=1)\n",
        "            else:\n",
        "                image_in = torch.cat([image_fls_in, image_in], dim=1)\n",
        "\n",
        "            # image_in, image_out = \\\n",
        "            #     image_in.reshape(-1, 6, 256, 256).to(device), image_out.reshape(-1, 3, 256, 256).to(device)\n",
        "\n",
        "            # image2image net fp\n",
        "            g_out = self.G(image_in)\n",
        "            g_out = torch.tanh(g_out)\n",
        "\n",
        "            loss_l1 = self.criterionL1(g_out, image_out)\n",
        "            loss_vgg, loss_style = self.criterionVGG(g_out, image_out, style=True)\n",
        "\n",
        "            loss_vgg, loss_style = torch.mean(loss_vgg), torch.mean(loss_style)\n",
        "\n",
        "            loss = loss_l1  + loss_vgg + loss_style\n",
        "            if(is_training):\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # log\n",
        "            if(self.opt_parser.write):\n",
        "                self.writer.add_scalar('loss', loss.cpu().detach().numpy(), self.count)\n",
        "                self.writer.add_scalar('loss_l1', loss_l1.cpu().detach().numpy(), self.count)\n",
        "                self.writer.add_scalar('loss_vgg', loss_vgg.cpu().detach().numpy(), self.count)\n",
        "                self.count += 1\n",
        "\n",
        "            # save image to track training process\n",
        "            if (i % self.opt_parser.jpg_freq == 0):\n",
        "                vis_in = np.concatenate([image_in[0, 3:6].cpu().detach().numpy().transpose((1, 2, 0)),\n",
        "                                         image_in[0, 0:3].cpu().detach().numpy().transpose((1, 2, 0))], axis=1)\n",
        "                vis_out = np.concatenate([image_out[0].cpu().detach().numpy().transpose((1, 2, 0)),\n",
        "                                          g_out[0].cpu().detach().numpy().transpose((1, 2, 0))], axis=1)\n",
        "                vis = np.concatenate([vis_in, vis_out], axis=0)\n",
        "                try:\n",
        "                    os.makedirs(os.path.join(self.opt_parser.jpg_dir, self.opt_parser.name))\n",
        "                except:\n",
        "                    pass\n",
        "                cv2.imwrite(os.path.join(self.opt_parser.jpg_dir, self.opt_parser.name, 'e{:03d}_b{:04d}.jpg'.format(epoch, i)), vis * 255.0)\n",
        "            # save ckpt\n",
        "            if (i % self.opt_parser.ckpt_last_freq == 0):\n",
        "                self.__save_model__('last', epoch)\n",
        "\n",
        "            print(\"Epoch {}, Batch {}/{}, loss {:.4f}, l1 {:.4f}, vggloss {:.4f}, styleloss {:.4f} time {:.4f}\".format(\n",
        "                epoch, i, len(self.dataset) // self.opt_parser.batch_size,\n",
        "                loss.cpu().detach().numpy(),\n",
        "                loss_l1.cpu().detach().numpy(),\n",
        "                loss_vgg.cpu().detach().numpy(),\n",
        "                loss_style.cpu().detach().numpy(),\n",
        "                          time.time() - st_batch))\n",
        "\n",
        "            g_time += time.time() - st_batch\n",
        "\n",
        "\n",
        "            if(self.opt_parser.test_speed):\n",
        "                if(i >= 100):\n",
        "                    break\n",
        "\n",
        "        print('Epoch time usage:', time.time() - st_epoch, 'I/O time usage:', time.time() - st_epoch - g_time, '\\n=========================')\n",
        "        if(self.opt_parser.test_speed):\n",
        "            exit(0)\n",
        "        if(epoch % self.opt_parser.ckpt_epoch_freq == 0):\n",
        "            self.__save_model__('{:02d}'.format(epoch), epoch)\n",
        "\n",
        "\n",
        "    def __save_model__(self, save_type, epoch):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.opt_parser.ckpt_dir, self.opt_parser.name))\n",
        "        except:\n",
        "            pass\n",
        "        if (self.opt_parser.write):\n",
        "            torch.save({\n",
        "            'G': self.G.state_dict(),\n",
        "            'opt': self.optimizer,\n",
        "            'epoch': epoch\n",
        "        }, os.path.join(self.opt_parser.ckpt_dir, self.opt_parser.name, 'ckpt_{}.pth'.format(save_type)))\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.opt_parser.nepoch):\n",
        "            self.__train_pass__(epoch, is_training=True)\n",
        "\n",
        "    def test(self):\n",
        "        if (self.opt_parser.use_vox_dataset == 'raw'):\n",
        "            if(self.opt_parser.add_audio_in):\n",
        "                from src.dataset.image_translation.image_translation_dataset import \\\n",
        "                    image_translation_raw98_with_audio_test_dataset as image_translation_test_dataset\n",
        "            else:\n",
        "                from src.dataset.image_translation.image_translation_dataset import image_translation_raw98_test_dataset as image_translation_test_dataset\n",
        "        else:\n",
        "            from src.dataset.image_translation.image_translation_dataset import image_translation_preprocessed98_test_dataset as image_translation_test_dataset\n",
        "        self.dataset = image_translation_test_dataset(num_frames=self.opt_parser.num_frames)\n",
        "        self.dataloader = torch.utils.data.DataLoader(self.dataset,\n",
        "                                                      batch_size=1,\n",
        "                                                      shuffle=True,\n",
        "                                                      num_workers=self.opt_parser.num_workers)\n",
        "\n",
        "        self.G.eval()\n",
        "        for i, batch in enumerate(self.dataloader):\n",
        "            print(i, 50)\n",
        "            if (i > 50):\n",
        "                break\n",
        "\n",
        "            if (self.opt_parser.add_audio_in):\n",
        "                image_in, image_out, audio_in = batch\n",
        "                audio_in = audio_in.reshape(-1, 1, 256, 256).to(device)\n",
        "            else:\n",
        "                image_in, image_out = batch\n",
        "\n",
        "            # # online landmark (AwingNet)\n",
        "            with torch.no_grad():\n",
        "                image_in, image_out = \\\n",
        "                    image_in.reshape(-1, 3, 256, 256).to(device), image_out.reshape(-1, 3, 256, 256).to(device)\n",
        "\n",
        "                pred_landmarks = []\n",
        "                for j in range(image_in.shape[0] // 16):\n",
        "                    inputs = image_out[j*16:j*16+16]\n",
        "                    outputs, boundary_channels = self.fa_model(inputs)\n",
        "                    pred_heatmap = outputs[-1][:, :-1, :, :].detach().cpu()\n",
        "                    pred_landmark, _ = get_preds_fromhm(pred_heatmap)\n",
        "                    pred_landmarks.append(pred_landmark.numpy() * 4)\n",
        "                pred_landmarks = np.concatenate(pred_landmarks, axis=0)\n",
        "\n",
        "            # draw landmark on while bg\n",
        "            img_fls = []\n",
        "            for pred_fl in pred_landmarks:\n",
        "                img_fl = np.ones(shape=(256, 256, 3)) * 255.0\n",
        "                img_fl = vis_landmark_on_img98(img_fl, pred_fl)  # 98x2\n",
        "                img_fls.append(img_fl.transpose((2, 0, 1)))\n",
        "            img_fls = np.stack(img_fls, axis=0).astype(np.float32) / 255.0\n",
        "            image_fls_in = torch.tensor(img_fls, requires_grad=False).to(device)\n",
        "\n",
        "            if (self.opt_parser.add_audio_in):\n",
        "                # print(image_fls_in.shape, image_in.shape, audio_in.shape)\n",
        "                image_in = torch.cat([image_fls_in,\n",
        "                                      image_in[0:image_fls_in.shape[0]],\n",
        "                                      audio_in[0:image_fls_in.shape[0]]], dim=1)\n",
        "            else:\n",
        "                image_in = torch.cat([image_fls_in, image_in[0:image_fls_in.shape[0]]], dim=1)\n",
        "\n",
        "            # normal 68 test dataset\n",
        "            # image_in, image_out = image_in.reshape(-1, 6, 256, 256), image_out.reshape(-1, 3, 256, 256)\n",
        "\n",
        "            # random single frame\n",
        "            # cv2.imwrite('random_img_{}.jpg'.format(i), np.swapaxes(image_out[5].numpy(),0, 2)*255.0)\n",
        "\n",
        "            image_in, image_out = image_in.to(device), image_out.to(device)\n",
        "\n",
        "            writer = cv2.VideoWriter('tmp_{:04d}.mp4'.format(i), cv2.VideoWriter_fourcc(*'mjpg'), 25, (256*4, 256))\n",
        "\n",
        "            for j in range(image_in.shape[0] // 16):\n",
        "                g_out = self.G(image_in[j*16:j*16+16])\n",
        "                g_out = torch.tanh(g_out)\n",
        "\n",
        "                # norm 68 pts\n",
        "                # g_out = np.swapaxes(g_out.cpu().detach().numpy(), 1, 3)\n",
        "                # ref_out = np.swapaxes(image_out[j*16:j*16+16].cpu().detach().numpy(), 1, 3)\n",
        "                # ref_in = np.swapaxes(image_in[j*16:j*16+16, 3:6, :, :].cpu().detach().numpy(), 1, 3)\n",
        "                # fls_in = np.swapaxes(image_in[j * 16:j * 16 + 16, 0:3, :, :].cpu().detach().numpy(), 1, 3)\n",
        "                g_out = g_out.cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "                g_out[g_out < 0] = 0\n",
        "                ref_out = image_out[j * 16:j * 16 + 16].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "                ref_in = image_in[j * 16:j * 16 + 16, 3:6, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "                fls_in = image_in[j * 16:j * 16 + 16, 0:3, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "\n",
        "                for k in range(g_out.shape[0]):\n",
        "                    frame = np.concatenate((ref_in[k], g_out[k], fls_in[k], ref_out[k]), axis=1) * 255.0\n",
        "                    writer.write(frame.astype(np.uint8))\n",
        "\n",
        "            writer.release()\n",
        "\n",
        "            os.system('ffmpeg -y -i tmp_{:04d}.mp4 -pix_fmt yuv420p random_{:04d}.mp4'.format(i, i))\n",
        "            os.system('rm tmp_{:04d}.mp4'.format(i))\n",
        "\n",
        "\n",
        "    def single_test(self, jpg=None, fls=None, filename=None, prefix='', grey_only=False):\n",
        "        import time\n",
        "        st = time.time()\n",
        "        self.G.eval()\n",
        "\n",
        "        if(jpg is None):\n",
        "            jpg = glob.glob1(self.opt_parser.single_test, '*.jpg')[0]\n",
        "            jpg = cv2.imread(os.path.join(self.opt_parser.single_test, jpg))\n",
        "\n",
        "        if(fls is None):\n",
        "            fls = glob.glob1(self.opt_parser.single_test, '*.txt')[0]\n",
        "            fls = np.loadtxt(os.path.join(self.opt_parser.single_test, fls))\n",
        "            fls = fls * 95\n",
        "            fls[:, 0::3] += 130\n",
        "            fls[:, 1::3] += 80\n",
        "\n",
        "        # writer = cv2.VideoWriter('out.mp4', cv2.VideoWriter_fourcc(*'mjpg'), 62.5, (540 * 1, 624))\n",
        "\n",
        "        # for i, frame in enumerate(fls):\n",
        "\n",
        "        #     img_fl = np.ones(shape=(256, 256, 3)) * 255\n",
        "        #     fl = frame.astype(int)\n",
        "        #     img_fl = vis_landmark_on_img(img_fl, np.reshape(fl, (68, 3)))\n",
        "        #     frame = np.concatenate((img_fl, jpg), axis=2).astype(np.float32)/255.0\n",
        "\n",
        "        #     image_in, image_out = frame.transpose((2, 0, 1)), np.zeros(shape=(3, 256, 256))\n",
        "        #     # image_in, image_out = frame.transpose((2, 1, 0)), np.zeros(shape=(3, 256, 256))\n",
        "        #     image_in, image_out = torch.tensor(image_in, requires_grad=False), \\\n",
        "        #                           torch.tensor(image_out, requires_grad=False)\n",
        "\n",
        "        #     image_in, image_out = image_in.reshape(-1, 6, 256, 256), image_out.reshape(-1, 3, 256, 256)\n",
        "        #     image_in, image_out = image_in.to(device), image_out.to(device)\n",
        "\n",
        "        #     g_out = self.G(image_in)\n",
        "        #     g_out = torch.tanh(g_out)\n",
        "\n",
        "        #     g_out = g_out.cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "        #     g_out[g_out < 0] = 0\n",
        "        #     ref_in = image_in[:, 3:6, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "        #     fls_in = image_in[:, 0:3, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "        #     # g_out = g_out.cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "        #     # g_out[g_out < 0] = 0\n",
        "        #     # ref_in = image_in[:, 3:6, :, :].cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "        #     # fls_in = image_in[:, 0:3, :, :].cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "\n",
        "        #     if(grey_only):\n",
        "        #         g_out_grey =np.mean(g_out, axis=3, keepdims=True)\n",
        "        #         g_out[:, :, :, 0:1] = g_out[:, :, :, 1:2] = g_out[:, :, :, 2:3] = g_out_grey\n",
        "\n",
        "\n",
        "        #     for i in range(g_out.shape[0]):\n",
        "        #         # frame = np.concatenate((ref_out[i], g_out[i], fls_in[i]), axis=1) * 255.0\n",
        "        #         frame = g_out[i] * 255.0\n",
        "\n",
        "        #         # import numpy as np\n",
        "        #         from PIL import Image\n",
        "\n",
        "        #         image = cv2.imread(\"/content/MakeItTalk/examples/girl_0.jpg\")\n",
        "        #         # numpy_array = np.random.rand(256, 256, 3)\n",
        "        #         image_pil = Image.fromarray(image.astype(np.uint8))\n",
        "        #         pil_image = Image.fromarray(frame.astype(np.uint8))\n",
        "\n",
        "        #         # new_image_pil = Image.new('RGB', (540, 624), (255, 255, 255))\n",
        "        #         # new_image_pil.paste(image_pil, (0, 0))\n",
        "\n",
        "        #         # image_pil = Image.fromarray(image)\n",
        "        #         image_pil.paste(pil_image, (face_locations[0][3]-50, face_locations[0][0]-50))\n",
        "\n",
        "        #         # pil_image = Image.open('example.jpg')\n",
        "        #         numpy_array = np.array(image_pil)\n",
        "                \n",
        "        #         writer.write(frame.astype(np.uint8))\n",
        "\n",
        "        #         # img = cv2.imread(\"/content/MakeItTalk/examples/girl_0.jpg\")\n",
        "        #         # writer.write(img)\n",
        "\n",
        "        # writer.release()\n",
        "        # print('Time - only video:', time.time() - st)\n",
        "\n",
        "        # if(filename is None):\n",
        "        #     filename = 'v'\n",
        "        # os.system('ffmpeg -loglevel error -y -i out.mp4 -i {} -pix_fmt yuv420p -strict -2 examples/{}_{}.mp4'.format(\n",
        "        #     'examples/'+filename[9:-16]+'.wav',\n",
        "        #     prefix, filename[:-4]))\n",
        "        # # os.system('rm out.mp4')\n",
        "\n",
        "        # print('Time - ffmpeg add audio:', time.time() - st)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        writer = cv2.VideoWriter('out.mp4', cv2.VideoWriter_fourcc(*'mjpg'), 62.5, (440 * 1, 509))\n",
        "\n",
        "        for i, frame in enumerate(fls):\n",
        "\n",
        "            img_fl = np.ones(shape=(256, 256, 3)) * 255\n",
        "            fl = frame.astype(int)\n",
        "            img_fl = vis_landmark_on_img(img_fl, np.reshape(fl, (68, 3)))\n",
        "            frame = np.concatenate((img_fl, jpg), axis=2).astype(np.float32)/255.0\n",
        "\n",
        "            image_in, image_out = frame.transpose((2, 0, 1)), np.zeros(shape=(3, 256, 256))\n",
        "            # image_in, image_out = frame.transpose((2, 1, 0)), np.zeros(shape=(3, 256, 256))\n",
        "            image_in, image_out = torch.tensor(image_in, requires_grad=False), \\\n",
        "                                  torch.tensor(image_out, requires_grad=False)\n",
        "\n",
        "            image_in, image_out = image_in.reshape(-1, 6, 256, 256), image_out.reshape(-1, 3, 256, 256)\n",
        "            image_in, image_out = image_in.to(device), image_out.to(device)\n",
        "\n",
        "            g_out = self.G(image_in)\n",
        "            g_out = torch.tanh(g_out)\n",
        "\n",
        "            g_out = g_out.cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "            g_out[g_out < 0] = 0\n",
        "            ref_in = image_in[:, 3:6, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "            fls_in = image_in[:, 0:3, :, :].cpu().detach().numpy().transpose((0, 2, 3, 1))\n",
        "            # g_out = g_out.cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "            # g_out[g_out < 0] = 0\n",
        "            # ref_in = image_in[:, 3:6, :, :].cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "            # fls_in = image_in[:, 0:3, :, :].cpu().detach().numpy().transpose((0, 3, 2, 1))\n",
        "\n",
        "            if(grey_only):\n",
        "                g_out_grey =np.mean(g_out, axis=3, keepdims=True)\n",
        "                g_out[:, :, :, 0:1] = g_out[:, :, :, 1:2] = g_out[:, :, :, 2:3] = g_out_grey\n",
        "\n",
        "\n",
        "            for i in range(g_out.shape[0]):\n",
        "                # # frame = np.concatenate((ref_in[i], g_out[i], fls_in[i]), axis=1) * 255.0\n",
        "                # frame = g_out[i] * 255.0\n",
        "                # writer.write(frame.astype(np.uint8))\n",
        "\n",
        "\n",
        "                # frame = np.concatenate((ref_out[i], g_out[i], fls_in[i]), axis=1) * 255.0\n",
        "                frame = g_out[i] * 255.0\n",
        "\n",
        "                # import numpy as np\n",
        "                from PIL import Image\n",
        "\n",
        "                image = cv2.imread(\"/content/MakeItTalk/examples/girl_0.jpg\")\n",
        "                # numpy_array = np.random.rand(256, 256, 3)\n",
        "                image_pil = Image.fromarray(image.astype(np.uint8))\n",
        "                pil_image = Image.fromarray(frame.astype(np.uint8))\n",
        "\n",
        "                # new_image_pil = Image.new('RGB', (540, 624), (255, 255, 255))\n",
        "                # new_image_pil.paste(image_pil, (0, 0))\n",
        "\n",
        "                # image_pil = Image.fromarray(image)\n",
        "                image_pil.paste(pil_image, (face_locations[0][3]-50, face_locations[0][0]-50))\n",
        "\n",
        "                # pil_image = Image.open('example.jpg')\n",
        "                numpy_array = np.array(image_pil)\n",
        "                \n",
        "                writer.write(numpy_array.astype(np.uint8))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        writer.release()\n",
        "        print('Time - only video:', time.time() - st)\n",
        "\n",
        "        if(filename is None):\n",
        "            filename = 'v'\n",
        "        os.system('ffmpeg -loglevel error -y -i out.mp4 -i {} -pix_fmt yuv420p -strict -2 examples/{}_{}.mp4'.format(\n",
        "            'examples/'+filename[9:-16]+'.wav',\n",
        "            prefix, filename[:-4]))\n",
        "        # os.system('rm out.mp4')\n",
        "\n",
        "        print('Time - ffmpeg add audio:', time.time() - st)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 837,
          "status": "ok",
          "timestamp": 1679432046100,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "uFChFT3I8zFI",
        "outputId": "9d2c1a89-fe8c-4209-a8bc-28fcf131bef0"
      },
      "outputs": [],
      "source": [
        "!identify /content/MakeItTalk/examples/girl_0.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNzY0KtMhkkV"
      },
      "source": [
        "## Step 4: Audio-to-Landmarks prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 643,
          "status": "ok",
          "timestamp": 1679431426431,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "FGyyJjJFYwwM",
        "outputId": "6f0c6b87-3077-4603-b849-8f469b3cb57e"
      },
      "outputs": [],
      "source": [
        "import face_recognition\n",
        "from PIL import Image\n",
        "\n",
        "# 加载图片\n",
        "image = face_recognition.load_image_file(\"/content/MakeItTalk/examples/girl_0.jpg\")\n",
        "\n",
        "# 定位脸部位置\n",
        "face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "# 裁剪出脸部图片\n",
        "face_image = Image.fromarray(image).crop((face_locations[0][3]-50, face_locations[0][0]-50, face_locations[0][3]+206, face_locations[0][0]+206))\n",
        "print(face_locations[0][3], face_locations[0][0], face_locations[0][1], face_locations[0][2])\n",
        "# 保存脸部图像\n",
        "face_image.save(\"/content/MakeItTalk/examples/girl_detail.jpg\")\n",
        "\n",
        "# 将脸部图片粘贴到原始图片上\n",
        "image_pil = Image.fromarray(image)\n",
        "image_pil.paste(face_image, (face_locations[0][3], face_locations[0][0]))\n",
        "\n",
        "# 保存新的图片\n",
        "image_pil.save(\"/content/MakeItTalk/examples/girl_1.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2629,
          "status": "ok",
          "timestamp": 1679431432516,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "HFWUlzj0lozq",
        "outputId": "b2f8df2e-4275-4d9a-fea1-06cfff193782"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "    model.test(au_emb=au_emb)\n",
        "else:\n",
        "    model.test(au_emb=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 845,
          "status": "ok",
          "timestamp": 1679426555606,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "-5vpK5oKalQ6",
        "outputId": "ae1922ee-ec15-4634-d32b-22099bf5dff3"
      },
      "outputs": [],
      "source": [
        "!identify /content/MakeItTalk/examples/girl_0.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 690,
          "status": "ok",
          "timestamp": 1679426559405,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "WP94GnGchXy8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "numpy_array = np.random.rand(256, 256, 3)\n",
        "pil_image = Image.fromarray((numpy_array * 255).astype(np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFaYlUNNjnxn"
      },
      "source": [
        "## Step 5: Natural face animation via Image-to-image translation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 13110,
          "status": "ok",
          "timestamp": 1679432087170,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "-xYBO_czjFSD",
        "outputId": "1861a326-0be2-41bd-c509-6e6cdade62ab"
      },
      "outputs": [],
      "source": [
        "fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
        "fls.sort()\n",
        "\n",
        "for i in range(0,len(fls)):\n",
        "    fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
        "    fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "    fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "    if (ADD_NAIVE_EYE):\n",
        "        fl = util.add_naive_eye(fl)\n",
        "\n",
        "    # additional smooth\n",
        "    fl = fl.reshape((-1, 204))\n",
        "    fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
        "    fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "    fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "    ''' STEP 6: Imag2image translation '''\n",
        "    model = Image_translation_block1(opt_parser, single_test=True)\n",
        "    with torch.no_grad():\n",
        "        model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
        "        print('finish image2image gen')\n",
        "    # os.remove(os.path.join('examples', fls[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8mMguI_j1TQ"
      },
      "source": [
        "## Visualize your animation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "executionInfo": {
          "elapsed": 792,
          "status": "ok",
          "timestamp": 1679432090671,
          "user": {
            "displayName": "Ttnn Wwaw",
            "userId": "03926079553968079319"
          },
          "user_tz": -480
        },
        "id": "Xmnr2CsChmnB",
        "outputId": "59d49cf8-56a0-448a-91ce-fc39c50c670f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "for ain in ains:\n",
        "  OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
        "    opt_parser.jpg.split('.')[0],\n",
        "    ain.split('.')[0]\n",
        "    )\n",
        "  mp4 = open('examples/{}'.format(OUTPUT_MP4_NAME),'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "  print('Display animation: examples/{}'.format(OUTPUT_MP4_NAME))\n",
        "  display(HTML(\"\"\"\n",
        "  <video width=300 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpyeNVY5Vnfm"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "    model.test(au_emb=au_emb)\n",
        "else:\n",
        "    model.test(au_emb=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxWMuEEbpywq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [
        {
          "file_id": "https://github.com/yzhou359/MakeItTalk/blob/main/quick_demo.ipynb",
          "timestamp": 1677932033364
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
